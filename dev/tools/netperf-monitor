#!/usr/bin/env python3
"""
netperf-monitor - Real-Time Test Monitoring Dashboard

Displays live netperf test metrics with terminal-based UI showing:
- Real-time throughput/latency graphs
- Progress bars for test duration
- Summary statistics (min/max/avg/p50/p95/p99)
- Multi-test dashboard view
- CPU utilization trends

Features:
- ANSI terminal-based UI (no external dependencies)
- Live metric streaming from netperf --demo-mode
- ASCII graphs and charts
- Progress tracking with ETA
- Prometheus/StatsD export (optional)
- Multi-test monitoring dashboard

Usage:
    netperf-monitor -H server -- -d send -l 60
    netperf-monitor --follow test.log
    netperf-monitor --dashboard tests.json
    netperf-monitor --export-prometheus :9090

Author: Netperf Modernization Project - Phase 3
License: MIT
Version: 1.0.0
"""

import sys
import os
import subprocess
import argparse
import time
import re
import json
import threading
from collections import deque
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any

# ANSI terminal codes
class Terminal:
    """ANSI terminal control"""
    
    # Cursor control
    CLEAR = '\033[2J'
    CLEAR_LINE = '\033[2K'
    HOME = '\033[H'
    HIDE_CURSOR = '\033[?25l'
    SHOW_CURSOR = '\033[?25h'
    
    # Colors
    RESET = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    
    @staticmethod
    def move_to(row: int, col: int) -> str:
        """Move cursor to position"""
        return f'\033[{row};{col}H'
    
    @staticmethod
    def up(n: int = 1) -> str:
        """Move cursor up"""
        return f'\033[{n}A'
    
    @staticmethod
    def down(n: int = 1) -> str:
        """Move cursor down"""
        return f'\033[{n}B'
    
    @staticmethod
    def clear_screen():
        """Clear screen and move home"""
        sys.stdout.write(Terminal.CLEAR + Terminal.HOME)
        sys.stdout.flush()
    
    @staticmethod
    def get_size() -> Tuple[int, int]:
        """Get terminal size (rows, cols)"""
        try:
            import shutil
            cols, rows = shutil.get_terminal_size()
            return (rows, cols)
        except:
            return (24, 80)


class Sparkline:
    """ASCII sparkline generator"""
    
    CHARS = '▁▂▃▄▅▆▇█'
    
    @classmethod
    def generate(cls, data: List[float], width: int = 20) -> str:
        """Generate sparkline from data"""
        if not data or len(data) < 2:
            return '▁' * width
        
        # Take last width points
        data = list(data)[-width:]
        
        # Normalize to 0-7 range
        min_val = min(data)
        max_val = max(data)
        if max_val == min_val:
            return cls.CHARS[0] * len(data)
        
        normalized = [(v - min_val) / (max_val - min_val) * 7 for v in data]
        return ''.join(cls.CHARS[int(n)] for n in normalized)


class ProgressBar:
    """ASCII progress bar"""
    
    @staticmethod
    def generate(progress: float, width: int = 40, show_percent: bool = True) -> str:
        """Generate progress bar
        
        Args:
            progress: 0.0 to 1.0
            width: Bar width in characters
            show_percent: Show percentage text
        """
        filled = int(progress * width)
        bar = '█' * filled + '░' * (width - filled)
        
        if show_percent:
            pct = f'{progress * 100:.1f}%'
            return f'[{bar}] {pct}'
        return f'[{bar}]'


class MetricStats:
    """Track statistics for a metric"""
    
    def __init__(self, max_samples: int = 1000):
        self.samples = deque(maxlen=max_samples)
        self.min = float('inf')
        self.max = float('-inf')
        self.sum = 0.0
        self.count = 0
    
    def add(self, value: float):
        """Add sample"""
        self.samples.append(value)
        self.min = min(self.min, value)
        self.max = max(self.max, value)
        self.sum += value
        self.count += 1
    
    def mean(self) -> float:
        """Calculate mean"""
        return self.sum / self.count if self.count > 0 else 0.0
    
    def current(self) -> float:
        """Get most recent value"""
        return self.samples[-1] if self.samples else 0.0
    
    def percentile(self, p: float) -> float:
        """Calculate percentile"""
        if not self.samples:
            return 0.0
        sorted_samples = sorted(self.samples)
        idx = int(len(sorted_samples) * p / 100.0)
        return sorted_samples[min(idx, len(sorted_samples) - 1)]


class TestMonitor:
    """Monitor a single netperf test"""
    
    def __init__(self, test_name: str, duration: int):
        self.test_name = test_name
        self.duration = duration
        self.start_time = time.time()
        
        self.throughput = MetricStats()
        self.latency = MetricStats()
        self.cpu_local = MetricStats()
        self.cpu_remote = MetricStats()
        
        self.running = False
        self.completed = False
        self.error = None
    
    def elapsed(self) -> float:
        """Get elapsed time in seconds"""
        return time.time() - self.start_time
    
    def progress(self) -> float:
        """Get progress as 0.0 to 1.0"""
        if self.duration <= 0:
            return 0.0
        return min(self.elapsed() / self.duration, 1.0)
    
    def remaining(self) -> float:
        """Get remaining time in seconds"""
        return max(0, self.duration - self.elapsed())
    
    def eta(self) -> str:
        """Get ETA string"""
        rem = self.remaining()
        if rem <= 0:
            return "Complete"
        return str(timedelta(seconds=int(rem)))
    
    def update(self, metrics: Dict[str, float]):
        """Update with new metrics"""
        if 'throughput' in metrics:
            self.throughput.add(metrics['throughput'])
        if 'latency' in metrics:
            self.latency.add(metrics['latency'])
        if 'cpu_local' in metrics:
            self.cpu_local.add(metrics['cpu_local'])
        if 'cpu_remote' in metrics:
            self.cpu_remote.add(metrics['cpu_remote'])


class MonitorDisplay:
    """Terminal UI display manager"""
    
    def __init__(self, refresh_rate: float = 0.5):
        self.refresh_rate = refresh_rate
        self.monitors = []
        self.running = False
    
    def add_monitor(self, monitor: TestMonitor):
        """Add test monitor"""
        self.monitors.append(monitor)
    
    def start(self):
        """Start display loop"""
        self.running = True
        Terminal.clear_screen()
        sys.stdout.write(Terminal.HIDE_CURSOR)
        sys.stdout.flush()
        
        try:
            while self.running:
                self.render()
                time.sleep(self.refresh_rate)
        finally:
            sys.stdout.write(Terminal.SHOW_CURSOR)
            sys.stdout.flush()
    
    def stop(self):
        """Stop display loop"""
        self.running = False
    
    def render(self):
        """Render display"""
        rows, cols = Terminal.get_size()
        
        # Move to home
        sys.stdout.write(Terminal.HOME)
        
        # Title
        title = f" Netperf Live Monitor "
        sys.stdout.write(Terminal.BOLD + Terminal.CYAN)
        sys.stdout.write(title.center(cols, '═'))
        sys.stdout.write(Terminal.RESET + '\n')
        
        # Current time
        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        sys.stdout.write(f" {now}\n")
        sys.stdout.write('─' * cols + '\n')
        
        # Render each monitor
        for i, monitor in enumerate(self.monitors):
            self._render_monitor(monitor, cols)
            if i < len(self.monitors) - 1:
                sys.stdout.write('─' * cols + '\n')
        
        sys.stdout.flush()
    
    def _render_monitor(self, monitor: TestMonitor, width: int):
        """Render single test monitor"""
        # Test name and status
        status = "Running" if monitor.running else ("Complete" if monitor.completed else "Error")
        status_color = Terminal.GREEN if monitor.running else (Terminal.BLUE if monitor.completed else Terminal.RED)
        
        sys.stdout.write(f"\n{Terminal.BOLD}{monitor.test_name}{Terminal.RESET}")
        sys.stdout.write(f" {status_color}[{status}]{Terminal.RESET}\n")
        
        # Progress bar
        progress = monitor.progress()
        bar = ProgressBar.generate(progress, width=min(60, width - 20))
        elapsed_str = str(timedelta(seconds=int(monitor.elapsed())))
        eta_str = monitor.eta()
        sys.stdout.write(f"  {bar}  {elapsed_str} / {eta_str}\n")
        
        # Metrics
        if monitor.throughput.count > 0:
            self._render_metric(
                "Throughput",
                monitor.throughput,
                "Mbps",
                monitor.throughput.samples,
                width
            )
        
        if monitor.latency.count > 0:
            self._render_metric(
                "Latency",
                monitor.latency,
                "us",
                monitor.latency.samples,
                width
            )
        
        if monitor.cpu_local.count > 0:
            self._render_metric(
                "CPU Local",
                monitor.cpu_local,
                "%",
                monitor.cpu_local.samples,
                width,
                max_val=100.0
            )
        
        if monitor.cpu_remote.count > 0:
            self._render_metric(
                "CPU Remote",
                monitor.cpu_remote,
                "%",
                monitor.cpu_remote.samples,
                width,
                max_val=100.0
            )
    
    def _render_metric(self, name: str, stats: MetricStats, unit: str,
                      samples: deque, width: int, max_val: Optional[float] = None):
        """Render metric with sparkline"""
        current = stats.current()
        mean = stats.mean()
        min_val = stats.min
        max_val_stat = stats.max
        p50 = stats.percentile(50)
        p95 = stats.percentile(95)
        p99 = stats.percentile(99)
        
        # Metric name and current value
        sys.stdout.write(f"\n  {Terminal.BOLD}{name:12s}{Terminal.RESET}: ")
        sys.stdout.write(f"{Terminal.GREEN}{current:>10.2f}{Terminal.RESET} {unit}")
        
        # Sparkline
        sparkline_width = min(40, width - 50)
        if len(samples) >= 2:
            sparkline = Sparkline.generate(list(samples), sparkline_width)
            sys.stdout.write(f"  {sparkline}")
        
        sys.stdout.write('\n')
        
        # Statistics
        sys.stdout.write(f"                ")
        sys.stdout.write(f"Min: {Terminal.BLUE}{min_val:.2f}{Terminal.RESET} ")
        sys.stdout.write(f"Avg: {Terminal.CYAN}{mean:.2f}{Terminal.RESET} ")
        sys.stdout.write(f"Max: {Terminal.RED}{max_val_stat:.2f}{Terminal.RESET}")
        sys.stdout.write(f"  P50: {p50:.2f}  P95: {p95:.2f}  P99: {p99:.2f}\n")


class NetperfMonitor:
    """Main monitoring coordinator"""
    
    def __init__(self, netperf_path: str = 'netperf', verbose: bool = False):
        self.netperf_path = netperf_path
        self.verbose = verbose
        self.display = MonitorDisplay()
    
    def monitor_live(self, host: str, netperf_args: List[str], duration: int = 60):
        """Monitor live netperf test"""
        
        # Create monitor
        monitor = TestMonitor(f"netperf -H {host}", duration)
        self.display.add_monitor(monitor)
        
        # Build command with --demo-mode for interim results
        cmd = [self.netperf_path, '-H', host]
        
        # Add demo mode if not present
        if '--' not in netperf_args:
            netperf_args.append('--')
        demo_idx = netperf_args.index('--')
        netperf_args.insert(demo_idx, '--demo-mode')
        
        cmd.extend(netperf_args)
        
        if self.verbose:
            print(f"Command: {' '.join(cmd)}", file=sys.stderr)
        
        # Start netperf process
        proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1
        )
        
        monitor.running = True
        
        # Start display in separate thread
        display_thread = threading.Thread(target=self.display.start)
        display_thread.start()
        
        # Parse output
        try:
            for line in proc.stdout:
                line = line.strip()
                if not line:
                    continue
                
                # Parse interim results
                # Expected format: Interim result: 9473.56 Throughput Mbits/sec
                if 'Interim result' in line:
                    parts = line.split()
                    if len(parts) >= 3:
                        try:
                            value = float(parts[2])
                            metric_name = parts[3].lower() if len(parts) > 3 else 'throughput'
                            
                            if 'throughput' in metric_name or 'mbits' in metric_name:
                                monitor.update({'throughput': value})
                            elif 'latency' in metric_name or 'usec' in metric_name:
                                monitor.update({'latency': value})
                        except (ValueError, IndexError):
                            pass
        
        except KeyboardInterrupt:
            proc.kill()
        
        finally:
            proc.wait()
            monitor.running = False
            monitor.completed = proc.returncode == 0
            
            # Keep display for a moment
            time.sleep(2)
            self.display.stop()
            display_thread.join()
    
    def monitor_file(self, filepath: str):
        """Monitor netperf output from file (follow mode)"""
        print(f"Following file: {filepath}")
        print("Press Ctrl+C to stop")
        
        # TODO: Implement file following with tail -f style
        # Parse existing output format and update display
        raise NotImplementedError("File monitoring not yet implemented")
    
    def dashboard(self, tests: List[Dict]):
        """Monitor multiple tests in dashboard view"""
        print(f"Starting dashboard with {len(tests)} tests")
        
        # TODO: Implement multi-test dashboard
        # Spawn multiple netperf processes and monitor all
        raise NotImplementedError("Dashboard mode not yet implemented")


def main():
    parser = argparse.ArgumentParser(
        description='Real-time netperf test monitoring',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Monitor live test
  %(prog)s -H 192.168.1.10 -- -d send -l 60
  
  # Follow log file
  %(prog)s --follow test.log
  
  # Dashboard mode
  %(prog)s --dashboard tests.json
  
  # With custom netperf path
  %(prog)s --netperf /usr/local/bin/netperf -H server -- -d send -l 30

Output Metrics:
  - Real-time throughput (Mbps)
  - Real-time latency (microseconds)
  - CPU utilization (local/remote)
  - Progress and ETA
  - Min/Max/Avg/P50/P95/P99 statistics
  - ASCII sparkline graphs

For more information, see dev/docs/MONITORING.md
        """
    )
    
    # Monitoring mode
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('-H', '--host', metavar='HOST',
                           help='Monitor live test to host')
    mode_group.add_argument('--follow', metavar='FILE',
                           help='Follow netperf output file')
    mode_group.add_argument('--dashboard', metavar='FILE',
                           help='Multi-test dashboard (JSON config)')
    
    # Configuration
    parser.add_argument('--netperf', default='netperf',
                       help='Path to netperf binary (default: netperf)')
    parser.add_argument('-l', '--length', type=int, default=60,
                       help='Test duration in seconds (default: 60)')
    parser.add_argument('--refresh', type=float, default=0.5,
                       help='Display refresh rate in seconds (default: 0.5)')
    
    # Output
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='Verbose output')
    parser.add_argument('--version', action='version', version='%(prog)s 1.0.0')
    
    # Netperf arguments
    parser.add_argument('netperf_args', nargs='*',
                       help='Arguments passed to netperf (after --)')
    
    args = parser.parse_args()
    
    # Create monitor
    monitor = NetperfMonitor(
        netperf_path=args.netperf,
        verbose=args.verbose
    )
    
    # Set refresh rate
    monitor.display.refresh_rate = args.refresh
    
    # Execute monitoring mode
    try:
        if args.host:
            monitor.monitor_live(args.host, args.netperf_args, args.length)
        elif args.follow:
            monitor.monitor_file(args.follow)
        elif args.dashboard:
            with open(args.dashboard, 'r') as f:
                tests = json.load(f)
            monitor.dashboard(tests)
    
    except KeyboardInterrupt:
        print("\n\nMonitoring stopped by user")
        return 0
    except Exception as e:
        print(f"\nError: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
