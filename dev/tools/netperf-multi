#!/usr/bin/env python3
"""
netperf-multi - Multi-Instance Netperf Test Runner

Coordinates multiple netperf instances running in parallel with automatic
result collection, CPU affinity management, and aggregated statistics.

Usage:
    netperf-multi -H HOST -n INSTANCES [options]

Examples:
    # Run 4 parallel OMNI tests (default)
    netperf-multi -H 192.168.1.100 -n 4

    # Run 8 parallel tests with CPU pinning
    netperf-multi -H server1 -n 8 --affinity

    # Run 16 parallel tests with auto-aggregation
    netperf-multi -H server1 -n 16 --aggregate

    # Run parallel tests with different starting CPUs
    netperf-multi -H server1 -n 4 --affinity --start-cpu 4

    # Custom test arguments for specific test pattern
    netperf-multi -H server1 -n 4 -- -t TCP_RR -r 1,1 -b 10

Author: Netperf Modernization Project
License: MIT
Version: 1.0.0
"""

import sys
import os
import subprocess
import argparse
import json
import time
import threading
import signal
from collections import defaultdict
from pathlib import Path

# Optional import for CPU affinity
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False


class NetperfInstance:
    """Represents a single netperf test instance."""
    
    def __init__(self, instance_id, host, test_type, duration, output_format,
                 netperf_args, cpu_affinity=None):
        self.instance_id = instance_id
        self.host = host
        self.test_type = test_type
        self.duration = duration
        self.output_format = output_format
        self.netperf_args = netperf_args
        self.cpu_affinity = cpu_affinity
        
        self.process = None
        self.returncode = None
        self.stdout = None
        self.stderr = None
        self.result = None
        self.start_time = None
        self.end_time = None
        
    def build_command(self, netperf_path):
        """Build netperf command line."""
        cmd = [netperf_path]
        cmd.extend(['-H', self.host])
        cmd.extend(['-t', self.test_type])
        cmd.extend(['-l', str(self.duration)])
        
        # Output format
        if self.output_format == 'json':
            cmd.append('-J')
        elif self.output_format == 'csv':
            cmd.extend(['-o', 'csv'])
        elif self.output_format == 'keyval':
            cmd.extend(['-o', 'keyval'])
        
        # CPU affinity (local side)
        if self.cpu_affinity is not None:
            cmd.extend(['-T', str(self.cpu_affinity)])
        
        # Additional netperf arguments (pass after -- for test-specific options)
        if self.netperf_args:
            cmd.append('--')
            cmd.extend(self.netperf_args)
        
        return cmd
    
    def start(self, netperf_path, barrier=None):
        """Start the netperf instance."""
        cmd = self.build_command(netperf_path)
        
        self.start_time = time.time()
        self.process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Apply CPU affinity using psutil if available
        if HAS_PSUTIL and self.cpu_affinity is not None:
            try:
                p = psutil.Process(self.process.pid)
                p.cpu_affinity([self.cpu_affinity])
            except Exception as e:
                print(f"Warning: Could not set CPU affinity for instance {self.instance_id}: {e}",
                      file=sys.stderr)
        
        return self.process
    
    def wait(self):
        """Wait for instance to complete and collect results."""
        if self.process is None:
            return
        
        self.stdout, self.stderr = self.process.communicate()
        self.returncode = self.process.returncode
        self.end_time = time.time()
        
        # Parse output based on format
        if self.returncode == 0:
            try:
                if self.output_format == 'json':
                    self.result = json.loads(self.stdout)
                elif self.output_format == 'keyval':
                    self.result = self._parse_keyval(self.stdout)
                elif self.output_format == 'csv':
                    self.result = self._parse_csv(self.stdout)
            except Exception as e:
                print(f"Warning: Could not parse output for instance {self.instance_id}: {e}",
                      file=sys.stderr)
                self.result = None
    
    def _parse_keyval(self, output):
        """Parse KEYVAL output format."""
        result = {}
        for line in output.strip().split('\n'):
            if '=' in line:
                key, value = line.split('=', 1)
                try:
                    # Try to convert to number
                    result[key.strip()] = float(value.strip())
                except ValueError:
                    result[key.strip()] = value.strip()
        return result
    
    def _parse_csv(self, output):
        """Parse CSV output format."""
        lines = output.strip().split('\n')
        if len(lines) < 2:
            return None
        
        # First line is header (if present)
        headers = lines[0].split(',')
        values = lines[-1].split(',')  # Last line has results
        
        result = {}
        for header, value in zip(headers, values):
            try:
                result[header.strip()] = float(value.strip())
            except ValueError:
                result[header.strip()] = value.strip()
        
        return result
    
    def is_success(self):
        """Check if instance completed successfully."""
        return self.returncode == 0 and self.result is not None
    
    def elapsed_time(self):
        """Get elapsed time in seconds."""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return None


class MultiNetperf:
    """Multi-instance netperf coordinator."""
    
    def __init__(self, host, instances, test_type='OMNI', duration=10,
                 output_format='keyval', netperf_path='netperf',
                 use_affinity=False, start_cpu=0, netperf_args=None,
                 stagger=0, verbose=False):
        self.host = host
        self.num_instances = instances
        self.test_type = test_type
        self.duration = duration
        self.output_format = output_format
        self.netperf_path = netperf_path
        self.use_affinity = use_affinity
        self.start_cpu = start_cpu
        self.netperf_args = netperf_args or []
        self.stagger = stagger
        self.verbose = verbose
        
        self.instances = []
        self.threads = []
        self.interrupted = False
        
        # Set up signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Handle interrupt signals."""
        print("\nInterrupted! Stopping all instances...", file=sys.stderr)
        self.interrupted = True
        self.stop_all()
        sys.exit(1)
    
    def _get_cpu_affinity(self, instance_id):
        """Get CPU affinity for instance."""
        if not self.use_affinity:
            return None
        
        if not HAS_PSUTIL:
            print("Warning: psutil not available, CPU affinity disabled", file=sys.stderr)
            return None
        
        # Distribute instances across CPUs starting from start_cpu
        cpu_count = psutil.cpu_count(logical=True)
        cpu = (self.start_cpu + instance_id) % cpu_count
        return cpu
    
    def create_instances(self):
        """Create all netperf instances."""
        self.instances = []
        
        for i in range(self.num_instances):
            cpu_affinity = self._get_cpu_affinity(i)
            
            instance = NetperfInstance(
                instance_id=i,
                host=self.host,
                test_type=self.test_type,
                duration=self.duration,
                output_format=self.output_format,
                netperf_args=self.netperf_args,
                cpu_affinity=cpu_affinity
            )
            
            self.instances.append(instance)
            
            if self.verbose:
                print(f"Created instance {i}: {self.test_type} to {self.host}"
                      f"{f' (CPU {cpu_affinity})' if cpu_affinity is not None else ''}")
    
    def start_all(self):
        """Start all instances."""
        print(f"Starting {self.num_instances} parallel {self.test_type} tests to {self.host}...")
        
        start_time = time.time()
        
        for i, instance in enumerate(self.instances):
            if self.interrupted:
                break
            
            instance.start(self.netperf_path)
            
            if self.verbose:
                print(f"Started instance {i} (PID {instance.process.pid})")
            
            # Stagger starts if requested
            if self.stagger > 0 and i < self.num_instances - 1:
                time.sleep(self.stagger)
        
        startup_time = time.time() - start_time
        
        if self.verbose:
            print(f"All instances started in {startup_time:.3f}s")
    
    def wait_all(self):
        """Wait for all instances to complete."""
        if self.verbose:
            print(f"Waiting for tests to complete (duration: {self.duration}s)...")
        
        # Wait for all instances
        for i, instance in enumerate(self.instances):
            instance.wait()
            
            if self.verbose:
                status = "✓" if instance.is_success() else "✗"
                elapsed = instance.elapsed_time()
                print(f"{status} Instance {i} completed in {elapsed:.2f}s")
    
    def stop_all(self):
        """Stop all running instances."""
        for instance in self.instances:
            if instance.process and instance.process.poll() is None:
                try:
                    instance.process.terminate()
                    instance.process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    instance.process.kill()
                except Exception:
                    pass
    
    def get_successful_results(self):
        """Get results from successful instances."""
        results = []
        for instance in self.instances:
            if instance.is_success():
                results.append(instance.result)
        return results
    
    def print_summary(self):
        """Print summary of all instances."""
        successful = sum(1 for i in self.instances if i.is_success())
        failed = self.num_instances - successful
        
        print(f"\n{'='*70}")
        print(f"Multi-Instance Test Summary")
        print(f"{'='*70}")
        print(f"Test Type:          {self.test_type}")
        print(f"Target Host:        {self.host}")
        print(f"Total Instances:    {self.num_instances}")
        print(f"Successful:         {successful}")
        print(f"Failed:             {failed}")
        print(f"Duration:           {self.duration}s")
        print(f"CPU Affinity:       {'Enabled' if self.use_affinity else 'Disabled'}")
        print(f"{'='*70}\n")
    
    def aggregate_results(self):
        """Aggregate results from all successful instances."""
        results = self.get_successful_results()
        
        if not results:
            print("No successful results to aggregate", file=sys.stderr)
            return None
        
        # Debug: Print first result
        if self.verbose:
            print(f"DEBUG: First result keys: {list(results[0].keys())}", file=sys.stderr)
        
        # Determine test type and aggregate appropriately
        aggregated = {
            'num_instances': len(results),
            'test_type': self.test_type
        }
        
        # Check if results contain throughput data
        throughput_key = self._find_throughput_key(results[0])
        if throughput_key:
            # For throughput tests (STREAM), sum the throughput
            total_throughput = sum(r.get(throughput_key, 0) for r in results)
            aggregated[throughput_key] = total_throughput
            aggregated['avg_per_instance'] = total_throughput / len(results)
        
        # Check if results contain latency data  
        latency_key = self._find_latency_key(results[0])
        rate_key = self._find_rate_key(results[0])
        if latency_key or rate_key:
            # For latency tests (RR), average the latency and sum transaction rate
            if latency_key:
                avg_latency = sum(r.get(latency_key, 0) for r in results) / len(results)
                aggregated[latency_key] = avg_latency
            
            if rate_key:
                total_rate = sum(r.get(rate_key, 0) for r in results)
                aggregated[rate_key] = total_rate
                aggregated['avg_per_instance'] = total_rate / len(results)
        for key in results[0].keys():
            if key not in aggregated:
                values = [r.get(key, 0) for r in results if isinstance(r.get(key), (int, float))]
                if values:
                    aggregated[f'{key}_avg'] = sum(values) / len(values)
                    aggregated[f'{key}_min'] = min(values)
                    aggregated[f'{key}_max'] = max(values)
        
        return aggregated
    
    def _find_throughput_key(self, result):
        """Find throughput key in results."""
        candidates = ['THROUGHPUT', 'throughput', 'Throughput_Mbps', 
                     'Throughput', 'THROUGHPUT_MBPS']
        for key in candidates:
            if key in result:
                return key
        return None
    
    def _find_latency_key(self, result):
        """Find latency key in results."""
        candidates = ['MEAN_LATENCY', 'mean_latency', 'Latency_us',
                     'Mean_Latency', 'latency']
        for key in candidates:
            if key in result:
                return key
        return None
    
    def _find_rate_key(self, result):
        """Find transaction rate key in results."""
        candidates = ['TRANSACTION_RATE', 'transaction_rate', 'Trans_Rate',
                     'Transactions_per_Second', 'trans_rate']
        for key in candidates:
            if key in result:
                return key
        return None
    
    def print_aggregated_results(self, aggregated):
        """Print aggregated results."""
        if not aggregated:
            return
        
        print("Aggregated Results:")
        print(f"{'='*70}")
        
        for key, value in aggregated.items():
            if isinstance(value, float):
                print(f"{key:30s}: {value:>15.2f}")
            else:
                print(f"{key:30s}: {value:>15}")
        
        print(f"{'='*70}\n")
    
    def export_results(self, output_file):
        """Export results to file."""
        results = self.get_successful_results()
        aggregated = self.aggregate_results()
        
        output = {
            'test_config': {
                'host': self.host,
                'instances': self.num_instances,
                'test_type': self.test_type,
                'duration': self.duration,
                'cpu_affinity': self.use_affinity,
                'start_cpu': self.start_cpu
            },
            'instance_results': results,
            'aggregated': aggregated
        }
        
        with open(output_file, 'w') as f:
            json.dump(output, f, indent=2)
        
        print(f"Results exported to {output_file}")
    
    def run(self, aggregate=True, export=None):
        """Run the multi-instance test."""
        self.create_instances()
        self.start_all()
        
        if not self.interrupted:
            self.wait_all()
            self.print_summary()
            
            if aggregate:
                aggregated = self.aggregate_results()
                self.print_aggregated_results(aggregated)
            
            if export:
                self.export_results(export)
            
            # Return success if all instances succeeded
            successful = sum(1 for i in self.instances if i.is_success())
            return successful == self.num_instances
        
        return False


def find_netperf():
    """Find netperf binary in PATH or standard locations."""
    # Check PATH
    import shutil
    netperf_path = shutil.which('netperf')
    if netperf_path:
        return netperf_path
    
    # Check standard locations
    standard_paths = [
        '/usr/bin/netperf',
        '/usr/local/bin/netperf',
        '/opt/netperf/build/src/netperf',
        './src/netperf',
        './netperf'
    ]
    
    for path in standard_paths:
        if os.path.isfile(path) and os.access(path, os.X_OK):
            return path
    
    return 'netperf'  # Fallback to 'netperf' and hope it's in PATH


def main():
    parser = argparse.ArgumentParser(
        description='Multi-instance netperf test runner',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s -H 192.168.1.100 -n 4
  %(prog)s -H server1 -n 8 --affinity
  %(prog)s -H server1 -n 16 --aggregate --export results.json
  %(prog)s -H server1 -n 4 -- -t TCP_RR -r 1,1 -b 10

For more information, see dev/docs/MULTI_INSTANCE.md
        """
    )
    
    # Required arguments
    parser.add_argument('-H', '--host', required=True,
                       help='Target netserver host')
    parser.add_argument('-n', '--instances', type=int, required=True,
                       help='Number of parallel instances')
    
    # Test configuration
    parser.add_argument('-t', '--test', dest='test_type', default='TCP_STREAM',
                       help='Test type (default: TCP_STREAM, use OMNI for modern)')
    parser.add_argument('-l', '--duration', type=int, default=10,
                       help='Test duration in seconds (default: 10)')
    parser.add_argument('-o', '--output', dest='output_format',
                       choices=['keyval', 'json', 'csv'],
                       default='keyval',
                       help='Output format (default: keyval)')
    
    # CPU affinity
    parser.add_argument('--affinity', action='store_true',
                       help='Enable CPU affinity (requires psutil)')
    parser.add_argument('--start-cpu', type=int, default=0,
                       help='Starting CPU for affinity (default: 0)')
    
    # Execution options
    parser.add_argument('--stagger', type=float, default=0,
                       help='Stagger start time between instances (seconds)')
    parser.add_argument('--netperf', dest='netperf_path',
                       help='Path to netperf binary (auto-detected if not specified)')
    
    # Output options
    parser.add_argument('--aggregate', action='store_true',
                       help='Print aggregated results')
    parser.add_argument('--no-aggregate', dest='aggregate',
                       action='store_false',
                       help='Do not print aggregated results')
    parser.add_argument('--export', metavar='FILE',
                       help='Export results to JSON file')
    
    # Verbosity
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='Verbose output')
    parser.add_argument('--version', action='version', version='%(prog)s 1.0.0')
    
    # Additional netperf arguments
    parser.add_argument('netperf_args', nargs='*',
                       help='Additional arguments passed to each netperf instance')
    
    args = parser.parse_args()
    
    # Validate instances
    if args.instances < 1:
        parser.error("Number of instances must be at least 1")
    
    # Check CPU affinity requirements
    if args.affinity and not HAS_PSUTIL:
        print("Warning: psutil not available, CPU affinity will be disabled",
              file=sys.stderr)
        print("Install psutil with: pip install psutil", file=sys.stderr)
    
    # Find netperf binary
    netperf_path = args.netperf_path or find_netperf()
    
    if args.verbose:
        print(f"Using netperf: {netperf_path}")
    
    # Create and run multi-instance test
    multi = MultiNetperf(
        host=args.host,
        instances=args.instances,
        test_type=args.test_type,
        duration=args.duration,
        output_format=args.output_format,
        netperf_path=netperf_path,
        use_affinity=args.affinity,
        start_cpu=args.start_cpu,
        netperf_args=args.netperf_args,
        stagger=args.stagger,
        verbose=args.verbose
    )
    
    success = multi.run(
        aggregate=args.aggregate,
        export=args.export
    )
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
